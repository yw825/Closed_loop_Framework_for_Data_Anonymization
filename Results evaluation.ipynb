{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_rel, wilcoxon, shapiro\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import gc\n",
    "import itertools\n",
    "from sklearn.utils import resample\n",
    "import ast\n",
    "import json\n",
    "import re\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import utils \n",
    "import model_train\n",
    "from constants import *\n",
    "import particle_swarm\n",
    "from scipy.stats import ttest_rel, shapiro, wilcoxon\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your folder path\n",
    "folder_path = '/Users/yusiwei/Library/CloudStorage/OneDrive-Personal/research/Fourth Year Paper/Experiments/2nd experiments/Experiment results/V2/V2_1_G_Mean/Tracking info/'\n",
    "\n",
    "# Get all files (not directories) in the folder\n",
    "all_files = [\n",
    "    f for f in os.listdir(folder_path)\n",
    "    if os.path.isfile(os.path.join(folder_path, f)) and f != '.DS_Store'\n",
    "]\n",
    "\n",
    "# Define a regex pattern to extract k, n_cluster, and model name\n",
    "pattern = r'track_info_k(\\d+)_ncluster(\\d+)_([A-Za-z]+)_round(\\d+)\\.csv?'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plots_folder = '/Users/yusiwei/Library/CloudStorage/OneDrive-Personal/research/Fourth Year Paper/Experiments/2nd experiments/Experiment results/V2/V2_1_G_Mean/Results'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Iterations Checking<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filedirectory = os.path.join(Plots_folder, 'Iterations results')\n",
    "os.makedirs(filedirectory, exist_ok=True)  # Make sure the folder exists\n",
    "\n",
    "for file in all_files:\n",
    "    info = utils.extract_info_from_filename(file, pattern)\n",
    "    print(f\"k: {info['k']}, n_cluster: {info['n_cluster']}, ML model: {info['model']}, Round: {info['round']}\")\n",
    "\n",
    "    # Load the CSV file\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    results_df = pd.read_csv(file_path)\n",
    "\n",
    "    # Create plot filenames\n",
    "    base_filename = f\"k{info['k']}_ncluster{info['n_cluster']}_{info['model']}\"\n",
    "    loss_plot_path = os.path.join(filedirectory, f\"{base_filename}_global_best_max_loss_through_iterations.png\")\n",
    "    combined_plot_path = os.path.join(filedirectory, f\"{base_filename}_violations_before_and_after_adjusting_through_iterations.png\")\n",
    "\n",
    "    # Save plots\n",
    "    utils.plot_global_best_so_far(\n",
    "        results_df, 'Entropy-Loss', 'Minimum Loss Function Value',\n",
    "        agg_func='mean', y_range=None, output_path=loss_plot_path,show_plot=True\n",
    "    )\n",
    "\n",
    "    utils.plot_global_best_so_far_combined(\n",
    "        results_df, \n",
    "        'total_num_violated_records_before_adjusting', 'Before adjusting',\n",
    "        'total_num_violated_records_after_adjusting', 'After adjusting', \n",
    "        agg_func=None, y_range=None, \n",
    "        color_1='red', color_2='blue', output_path=combined_plot_path,show_plot=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Comparison Checking -- Maximum Loss<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_column = 'Entropy-Loss'\n",
    "\n",
    "summary_df = utils.summarize_global_best_across_files(folder_path, pattern, metric_column)\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_max_loss_dict = {\n",
    "    'DT': 14.0570,\n",
    "    'LR': 0.5762,\n",
    "    'NB': 2.2771,\n",
    "    'NN': 3.4248,\n",
    "    'RF': 0.6991,\n",
    "    'SVM': 0.6920\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filedirectory = os.path.join(Plots_folder, 'Compared to baseline')\n",
    "os.makedirs(filedirectory, exist_ok=True)  # Make sure the folder exists\n",
    "\n",
    "utils.plot_model_performance(summary_df, baseline_max_loss_dict, output_path=filedirectory,show_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Comparison Checking -- Average Loss<h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # German credit dataset\n",
    "# baseline_avg_loss_dict = {\n",
    "#     'DT': 11.2059,\n",
    "#     'LR': 0.4975,\n",
    "#     'NB': 1.3150,\n",
    "#     'NN': 7.1022,\n",
    "#     'RF': 0.4919,\n",
    "#     'SVM': 0.6318\n",
    "# }\n",
    "\n",
    "# Adult dataset\n",
    "baseline_avg_loss_dict = {\n",
    "    'DT': 6.7000,\n",
    "    'LR': 0.5139,\n",
    "    'NB': 1.0142,\n",
    "    'NN': 8.9624,\n",
    "    'RF': 0.3835,\n",
    "    'SVM': 0.6938\n",
    "}\n",
    "\n",
    "# # Sepsis dataset\n",
    "# baseline_avg_loss_dict = {\n",
    "#     'DT': 5.0402,\n",
    "#     'LR': 0.2183,\n",
    "#     'NB': 3.1888,\n",
    "#     'NN': 0.2537,\n",
    "#     'RF': 0.2376,\n",
    "#     'SVM': 0.2832\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_column = 'Entropy-Loss'\n",
    "\n",
    "summary_df = utils.summarize_global_best_across_files(folder_path, pattern, metric_column, agg_func='mean')\n",
    "print(summary_df)\n",
    "\n",
    "# Save the summary DataFrame to a CSV file\n",
    "summary_file_path = os.path.join(Plots_folder, 'global_best_loss_each_round_summary.csv')\n",
    "summary_df.to_csv(summary_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_plot_path = os.path.join(Plots_folder, 'Global best results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in summary_df['round'].unique():\n",
    "    print(f\"Round: {i}\")\n",
    "    round_df = summary_df[summary_df['round'] == i]\n",
    "    utils.plot_model_performance(round_df, baseline_avg_loss_dict, output_path=None,show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_df = summary_df.groupby(['k', 'model', 'n_clusters_set']).agg(\n",
    "    global_best=('global_best', 'mean'),\n",
    "    n_cluster=('n_cluster', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# Save the aggregated DataFrame to a CSV file\n",
    "agg_file_path = os.path.join(Plots_folder, 'global_best_loss_average_summary.csv')\n",
    "agg_df.to_csv(agg_file_path, index=False)\n",
    "\n",
    "print(agg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_model_performance(agg_df, baseline_avg_loss_dict, output_path=new_plot_path,show_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_df = pd.read_excel('/Users/yusiwei/Library/CloudStorage/OneDrive-Personal/research/Fourth Year Paper/Experiments/2nd experiments/Baseline experiment results/German_credit_baseline_ML_results.xlsm', sheet_name='German_credit_baseline_ML_resul')\n",
    "baseline_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "\n",
    "for file in all_files:\n",
    "    info = utils.extract_info_from_filename(file, pattern)\n",
    "    print(f\"k: {info['k']}, n_cluster: {info['n_cluster']}, ML model: {info['model']}, Round: {info['round']}\")\n",
    "\n",
    "    # Load the CSV file\n",
    "    file_path = os.path.join(folder_path, file)\n",
    "    results_df = pd.read_csv(file_path)\n",
    "\n",
    "    # Get global best loss values\n",
    "    loss_values = utils.get_metric_values_of_global_best_particle(results_df, 'Entropy-Loss', agg_func='mean')\n",
    "    mean_global_best = np.mean(loss_values)\n",
    "    # Print results\n",
    "    print(f\"Mean Loss - Proposed moddel: {mean_global_best:.4f}\")\n",
    "\n",
    "    # Get baseline loss\n",
    "    baseline_loss = np.array(baseline_df.loc[baseline_df['ML_Model'] == info['model'], 'Loss'])\n",
    "    mean_baseline = np.mean(baseline_loss)\n",
    "    print(f\"Mean Loss - Baseline: {mean_baseline:.4f}\")\n",
    "\n",
    "    # Check normality\n",
    "    differences = loss_values - baseline_loss\n",
    "    stat, p = shapiro(differences)\n",
    "\n",
    "    if p > 0.05:\n",
    "        print(\"✅ Differences are normally distributed. Proceed with paired t-test.\")\n",
    "        t_stat, p_val = ttest_rel(loss_values, baseline_loss)\n",
    "        # print(f\"t-statistic: p-value: {p_val}\")\n",
    "        if p_val < 0.05:\n",
    "            print(\"❌ Reject the null hypothesis: The means are significantly different.\")\n",
    "        else:\n",
    "            print(\"✅ Fail to reject the null hypothesis: The means are not significantly different.\")\n",
    "    else:\n",
    "        print(\"❌ Differences are NOT normally distributed. Use Wilcoxon signed-rank test.\")\n",
    "        t_stat, p_val = wilcoxon(loss_values, baseline_loss)\n",
    "        # print(f\"Wilcoxon-statistic: p-value: {p_val}\")\n",
    "        if p_val < 0.05:\n",
    "            print(\"❌ Reject the null hypothesis: The means are significantly different.\")\n",
    "        else:\n",
    "            print(\"✅ Fail to reject the null hypothesis: The means are not significantly different.\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Save the results to a data frame\n",
    "    results_list.append({\n",
    "        'Model': info['model'],\n",
    "        'k': info['k'],\n",
    "        'n_cluster': info['n_cluster'],\n",
    "        'Mean Loss - Proposed model': mean_global_best,\n",
    "        'Mean Loss - Baseline': mean_baseline,\n",
    "        'p-value': p_val\n",
    "    })\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_stats_df = pd.DataFrame(results_list)\n",
    "\n",
    "summary_stats_df.to_csv(os.path.join(Plots_folder, 'summary_stats.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
